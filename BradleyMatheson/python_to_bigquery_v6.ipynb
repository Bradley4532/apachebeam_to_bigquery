{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project: Import csv to bigquery table\n",
    "#StartDate: 4/06/2022\n",
    "#EndDate: \n",
    "#Developer: Bradley, Hongquy, Khoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,render_template,request,redirect,url_for\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "import os, re, datetime, uuid, time, json\n",
    "import google.cloud.logging\n",
    "import apache_beam as beam\n",
    "import argparse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from werkzeug.utils import secure_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to allow a authenticated user on the project to work on bigquerry\n",
    "Key_path = r\"C:\\\\Users\\Brad\\Documents\\model-craft-342921-ea36cdb339e7.json\"\n",
    "\n",
    "#this is needed to request a job\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = Key_path\n",
    "credentials = service_account.Credentials.from_service_account_file(Key_path,scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "client=bigquery.Client(credentials=credentials,project=credentials.project_id)\n",
    "#table_id='model-craft-342921.testing.Task3'\n",
    "storage_client=storage.Client(credentials=credentials,project=credentials.project_id)\n",
    "bucket = storage_client.get_bucket(\"data_intake4062022\")\n",
    "client1 = google.cloud.logging.Client()\n",
    "logger = client1.logger(name=\"dataflow.googleapis.com%2Fworker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterRecord(beam.DoFn):\n",
    "    def process(self, element, table_sch):\n",
    "        import apache_beam as beam\n",
    "        flag = 0\n",
    "        error = ''\n",
    "        if(len(element) > len(table_sch)):\n",
    "            flag = 1\n",
    "            error = 'There are too many columns. Expected {} columns, not {}'.format(len(table_sch),len(element))\n",
    "        else:\n",
    "            for x in range(0, len(element)):\n",
    "                #print(table_sch[x]['type'])\n",
    "                if(table_sch[x]['type'] == 'STRING'):\n",
    "                    if(element[x].strip().isalpha() == False):\n",
    "                        #flag = 1\n",
    "                        #error = 'Expected string not int data type in column {}'.format(table_sch[x]['name'])\n",
    "                        pass\n",
    "                    else:\n",
    "                        pass\n",
    "                elif(table_sch[x]['type'] == 'INTEGER'):\n",
    "                    if(element[x].strip().isalpha()):\n",
    "                        flag = 1\n",
    "                        error = 'Expected int not string data type in column: {}'.format(table_sch[x]['name'])\n",
    "                    elif('.' in element[x]):\n",
    "                        flag = 1\n",
    "                        error = 'Expected int not float data type in column: {}'.format(table_sch[x]['name'])\n",
    "                    else:\n",
    "                        pass\n",
    "                elif(table_sch[x]['type'] == 'FLOAT'):\n",
    "                    if(element[x].strip().isalpha()):\n",
    "                        flag = 1\n",
    "                        error = 'Expected float not string data type in column: {}'.format(table_sch[x]['name'])\n",
    "                    elif('.' in element[x] == False):\n",
    "                        flag = 1\n",
    "                        error = 'Expected float not int data type in column: {}'.format(table_sch[x]['name'])\n",
    "                    else:\n",
    "                        pass\n",
    "                if(table_sch[x]['mode'] == 'REQUIRED'):\n",
    "                        if(len(element[x]) == 0):\n",
    "                            flag = 1\n",
    "                            error = 'Null in required column: {}'.format(table_sch[x]['name'])\n",
    "        if(flag == 0):\n",
    "            yield beam.pvalue.TaggedOutput('Good', element)\n",
    "            yield beam.pvalue.TaggedOutput('Tgood', 1)\n",
    "        else:\n",
    "            error_log = \"\"\"| Error Message - {} || Inputted Data - {} |\\n\"\"\".format(error, element)\n",
    "            yield beam.pvalue.TaggedOutput('Bad', error_log)\n",
    "            yield beam.pvalue.TaggedOutput('Tbad', 1)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class formating(beam.DoFn):\n",
    "    def process(self, element, table_sch):\n",
    "        import apache_beam as beam\n",
    "        record = {}\n",
    "        for x in range(0, len(element)):\n",
    "            name = table_sch[x]['name']\n",
    "            record[name] = element[x]\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class converter(beam.DoFn):\n",
    "    def process(self, element, total_records, job_name,countBad):\n",
    "        import apache_beam as beam\n",
    "        record = {}\n",
    "        record['Project_id'] = str(job_name)\n",
    "        record['Total_records'] = int(total_records)\n",
    "        record['Total_good_records'] = int(element)\n",
    "        record['Total_bad_records'] = int(countBad)\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logging_message(beam.DoFn):\n",
    "    def process(self, element, filter_str):\n",
    "        import apache_beam as beam\n",
    "        messages = []\n",
    "        for entry in logger.list_entries(filter_=filter_str):  # API call(s)\n",
    "            message = entry.to_api_repr()['jsonPayload']['message']\n",
    "            list_extract = message.replace(\"There were errors inserting to BigQuery. Will not retry. Errors were \",\"\")\n",
    "            #print(list_extract)\n",
    "            list_extract = list_extract.replace(\"'\",'\"')\n",
    "            #print(list_extract)\n",
    "            res = json.loads(list_extract)\n",
    "            for i in range(0,len(res)):\n",
    "                messages.append(res[i]['errors'][0]['message'])\n",
    "        yield messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import google.cloud.logging\n",
    "import json\n",
    "#client = google.cloud.logging.Client(credentials=credentials,project=credentials.project_id)\n",
    "#logging_client = logging.Client()\n",
    "job_name = \"pythontobigquerry-22783b3c-3f33-47e1-bd2d-416297646a1b\"\n",
    "filter_str = (\n",
    "    f'resource.labels.job_name={job_name}'\n",
    "    f' resource.type=\"dataflow_step\"'\n",
    "    f' AND severity >= ERROR'\n",
    ")\n",
    "client = google.cloud.logging.Client()\n",
    "logger = client.logger(name=\"dataflow.googleapis.com%2Fworker\")\n",
    "#for entry in client.list_entries(filter_=filter_str):\n",
    "#    print(entry)\n",
    "messages = []\n",
    "for entry in logger.list_entries(filter_=filter_str):  # API call(s)\n",
    "    message = entry.to_api_repr()['jsonPayload']['message']\n",
    "    list_extract = message.replace(\"There were errors inserting to BigQuery. Will not retry. Errors were \",\"\")\n",
    "    #print(list_extract)\n",
    "    list_extract = list_extract.replace(\"'\",'\"')\n",
    "    #print(list_extract)\n",
    "    res = json.loads(list_extract)\n",
    "    for i in range(0,len(res)):\n",
    "        messages.append(res[i]['errors'][0]['message'])\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is needed to run the website html\n",
    "app = Flask(__name__)\n",
    "\n",
    "#secret key is to allow this python code to move variable from the frontend to the backend\n",
    "app.secret_key = \"35367565\"\n",
    "\n",
    "@app.route('/')\n",
    "def form():\n",
    "    return redirect(url_for('upload'))\n",
    "\n",
    "#This will render the frontend of the website to get the json file\n",
    "@app.route('/upload', methods=['GET','POST'])\n",
    "def upload():\n",
    "\n",
    "    #resets the message so that previous messages dont confuse the user\n",
    "    message = None\n",
    "    \n",
    "    tables = ['Task3'] #change--------------------------\n",
    "    \n",
    "    job_name = 'pythontobigquerry-{}'.format(uuid.uuid4())\n",
    "\n",
    "    beam_options = PipelineOptions(\n",
    "                                runner = 'DataflowRunner',\n",
    "                                #runner='DirectRunner',\n",
    "                                project = 'model-craft-342921',\n",
    "                                job_name = '{}'.format(job_name),\n",
    "                                temp_location = 'gs://data_intake4062022/temp1',\n",
    "                                region='us-east1',\n",
    "                                service_account_email = 'practice-py@model-craft-342921.iam.gserviceaccount.com'\n",
    "                            )\n",
    "    filter_str = (\n",
    "        f'resource.labels.job_name={job_name}'\n",
    "        f' resource.type=\"dataflow_step\"'\n",
    "        f' AND severity >= ERROR'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #This will only run if the user attempts to submit a file\n",
    "    if request.method == 'POST':\n",
    "\n",
    "        #this will only run if what the user submitted is a file\n",
    "        if 'file' in request.files:\n",
    "            #this gets the file data\n",
    "            file = request.files['file']\n",
    "            #this aquires the name of the file\n",
    "            filename = secure_filename(file.filename)\n",
    "            if len(file.readlines()) == 0:\n",
    "                message = \"File has no data to process\"\n",
    "            else:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    #this will only run if the file is a csv\n",
    "                    if filename.endswith('.csv'):\n",
    "                        blob = bucket.blob('data.csv')\n",
    "                        blob.upload_from_file(file)\n",
    "                        \n",
    "                        file.seek(0)\n",
    "                        total_records = len(file.readlines())\n",
    "                        \n",
    "                        table_id = request.form.getlist('checks')[0]\n",
    "                        SCHEMA = {}\n",
    "                        table_sch = []\n",
    "                        try:\n",
    "                            SchemaJob = client.get_table('model-craft-342921.testing.{}'.format(table_id))\n",
    "                            for s in SchemaJob.schema:\n",
    "                                new_dict = {}\n",
    "                                new_dict['name'] = s.name\n",
    "                                new_dict['type'] = s.field_type\n",
    "                                new_dict['mode'] = s.mode\n",
    "                                table_sch.append(new_dict)\n",
    "                            SCHEMA['fields'] = table_sch\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                        \n",
    "                        filename = \"LOG: \"+ str(datetime.datetime.now()) + \".txt\"\n",
    "                        try:\n",
    "                            #start = time.time()\n",
    "                            p = beam.Pipeline(options=beam_options)\n",
    "                            events  = (p | 'ReadData' >> beam.io.ReadFromText('gs://data_intake4062022/data.csv', skip_header_lines =1)\n",
    "                                   | 'Split' >> beam.Map(lambda x: x.split(','))\n",
    "                                    | 'format to dict2' >> beam.ParDo(formating(),table_sch) \n",
    "                                   | 'WriteToBigQuery2' >>  beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "                                       'model-craft-342921:testing.{}'.format(table_id),\n",
    "                                       schema=SCHEMA,\n",
    "                                       write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                                       insert_retry_strategy=beam.io.gcp.bigquery_tools.RetryStrategy.RETRY_NEVER,\n",
    "                                       method='STREAMING_INSERTS')\n",
    "                                    )\n",
    "                            tester = (events[beam.io.gcp.bigquery.BigQueryWriteFn.FAILED_ROWS]\n",
    "                                    | \"Bad lines\" >> beam.io.WriteToText('gs://practice_error_logs/{}'.format(filename), shard_name_template = \"\")\n",
    "                                    | \"Error Message\" >> beam.ParDo(logging_message(),filter_str)\n",
    "                                     )\n",
    "                            (tester | \"Store messages\" >> beam.io.WriteToText('gs://error_messages4142022/{}'.format(filename), shard_name_template = \"\")\n",
    "                            )\n",
    "                            \n",
    "                            result = p.run()\n",
    "                            #result.wait_until_finish()\n",
    "                            #end = time.time()\n",
    "                            #Total_time = end - start\n",
    "                            #string = \"\"\"UPDATE `model-craft-342921.testing.Statics` SET Completion_time = '{}' WHERE Project_id = '{}'\"\"\".format(Total_time, job_name)\n",
    "                            #query_job = client.query(string)\n",
    "                            #query_job.result()\n",
    "                            message = \"Data uploaded to the Bigquery\"\n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up the pipeline\"\n",
    "\n",
    "\n",
    "                    #If the file is not a json or the csv this will run\n",
    "                    else:\n",
    "                        message = \"File type is not excepted\"\n",
    "                    #endif\n",
    "                except Exception as error:\n",
    "                    print('This was the error: ', error)\n",
    "                    message = \"There was an error in creating the request\"\n",
    "            #endif\n",
    "        #This will run if the submition is not a file type\n",
    "        elif 'file' not in request.files:\n",
    "            message = \"There was no file to upload\"\n",
    "        #endif\n",
    "    #endif\n",
    "\n",
    "    #this will render the template on the website\n",
    "    return render_template(\"front.html\", message = message, tables = tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [14/Apr/2022 16:00:52] \"\u001b[32mGET / HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [14/Apr/2022 16:00:52] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [14/Apr/2022 16:00:52] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad\\anaconda3\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery.py:2138: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  is_streaming_pipeline = p.options.view_as(StandardOptions).streaming\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-494de0af-d83a-47c9-9495-33a41537f2f0.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-494de0af-d83a-47c9-9495-33a41537f2f0.json']\n",
      "127.0.0.1 - - [14/Apr/2022 16:01:22] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [14/Apr/2022 16:01:22] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
