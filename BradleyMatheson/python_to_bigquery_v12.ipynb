{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project: Import csv to bigquery table\n",
    "#StartDate: 4/06/2022\n",
    "#EndDate: \n",
    "#Developer: Bradley, Hongquy, Khoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: avro is a encoded file, so getting the total rows of the input file is currently not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,render_template,request,redirect,url_for\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "import os, re, datetime, uuid, time, json\n",
    "import google.cloud.logging\n",
    "import apache_beam as beam\n",
    "from io import StringIO\n",
    "import argparse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from werkzeug.utils import secure_filename\n",
    "import avro\n",
    "from avro.datafile import DataFileWriter, DataFileReader\n",
    "from avro.io import DatumWriter, DatumReader\n",
    "from apache_beam.io import ReadFromAvro\n",
    "from apache_beam.io import WriteToAvro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'model-craft-342921'\n",
    "temp_location = 'gs://data_intake4062022/temp1'\n",
    "region='us-east1'\n",
    "service_account_email = 'practice-py@model-craft-342921.iam.gserviceaccount.com'\n",
    "dataset = 'testing'\n",
    "data_input = 'gs://data_intake4062022'\n",
    "message_ouput = 'gs://complete_message_4182022'\n",
    "bad_rows_input = 'gs://practice_error_logs'\n",
    "local_file_loc = r\"C:\\\\Users\\Brad\\temp.txt\"\n",
    "error_message = 'gs://error_messages4142022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to allow a authenticated user on the project to work on bigquerry\n",
    "Key_path = r\"C:\\\\Users\\Brad\\Documents\\model-craft-342921-ea36cdb339e7.json\"\n",
    "\n",
    "#this is needed to request a job\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = Key_path\n",
    "credentials = service_account.Credentials.from_service_account_file(Key_path,scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "client=bigquery.Client(credentials=credentials,project=credentials.project_id)\n",
    "\n",
    "#this is needed to allow the request to access gcp cloud storage\n",
    "storage_client=storage.Client(credentials=credentials,project=credentials.project_id)\n",
    "\n",
    "#this is where a temporary file is stored to be accessed by the cloud\n",
    "bucket = storage_client.get_bucket(\"data_intake4062022\")\n",
    "\n",
    "#this is where the column location and message from bigquery is stored to be accessed later\n",
    "bucket1 = storage_client.get_bucket(\"error_messages4142022\")\n",
    "\n",
    "#this is where the rows attempted to be inserted into bigquery is stored to be accessed later\n",
    "bucket2 = storage_client.get_bucket(\"practice_error_logs\")\n",
    "\n",
    "bucket3 = storage_client.get_bucket(\"complete_message_4182022\")\n",
    "\n",
    "#this is needed to access logs from the bigquery job\n",
    "client1 = google.cloud.logging.Client()\n",
    "logger = client1.logger(name=\"dataflow.googleapis.com%2Fworker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a function created to format the input data into a dictionary so the data can be easily inserted into bigquery\n",
    "class formating_csv(beam.DoFn):\n",
    "    #table_sch is a side input that was found for the table we are trying to insert the data into\n",
    "    def process(self, element, table_sch, job_name):\n",
    "        import apache_beam as beam\n",
    "        record = {}\n",
    "        record['job_id'] = job_name\n",
    "        for x in range(0, len(element)):\n",
    "            if x < len(table_sch)-1:\n",
    "                name = table_sch[x]['name']\n",
    "                if element[x] == '':\n",
    "                    record[name] = None\n",
    "                else:\n",
    "                    record[name] = element[x]\n",
    "            else:\n",
    "                record['unknown{}'.format(x)] = element[x]\n",
    "        yield beam.pvalue.TaggedOutput('rec', record)\n",
    "        yield beam.pvalue.TaggedOutput('count', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class formating_avro(beam.DoFn):\n",
    "    #table_sch is a side input that was found for the table we are trying to insert the data into\n",
    "    def process(self, element, table_sch, job_name):\n",
    "        import apache_beam as beam\n",
    "        record = {}\n",
    "        record['job_id'] = job_name\n",
    "        for x in range(0, len(element)):\n",
    "            if x < len(table_sch)-1:\n",
    "                name = table_sch[x]['name']\n",
    "                if element[name] == '':\n",
    "                    record[name] = None\n",
    "                else:\n",
    "                    record[name] = element[name]\n",
    "            else:\n",
    "                record['unknown{}'.format(name)] = element[name]\n",
    "        yield beam.pvalue.TaggedOutput('rec', record)\n",
    "        yield beam.pvalue.TaggedOutput('count', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customJsonParse(beam.DoFn):\n",
    "    def process (self, json_file,job_name):\n",
    "        import apache_beam as beam\n",
    "        import json\n",
    "        data = []\n",
    "        for i in json_file.splitlines():\n",
    "            d = i.strip()\n",
    "            d = d.strip(',')\n",
    "            try:\n",
    "                d = json.loads(d)\n",
    "                d['job_id'] = job_name\n",
    "                data.append(d)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if data:\n",
    "            for row_data in data:\n",
    "                yield beam.pvalue.TaggedOutput('rec', row_data)\n",
    "                yield beam.pvalue.TaggedOutput('count', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class formating_parquet(beam.DoFn):\n",
    "    #table_sch is a side input that was found for the table we are trying to insert the data into\n",
    "    def process(self, element, job_name):\n",
    "        import apache_beam as beam\n",
    "        element['job_id'] = job_name\n",
    "        yield beam.pvalue.TaggedOutput('rec', element)\n",
    "        yield beam.pvalue.TaggedOutput('count', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class formating2(beam.DoFn):\n",
    "    def process(self, element, table_id):\n",
    "        import apache_beam as beam\n",
    "        element = element[1]\n",
    "        element.pop('job_id', None)\n",
    "        yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_setup(job_name):\n",
    "    return PipelineOptions(\n",
    "                            #runner = 'DataflowRunner',\n",
    "                            #runner='DirectRunner',\n",
    "                            project = project,\n",
    "                            job_name = '{}'.format(job_name),\n",
    "                            temp_location = temp_location,\n",
    "                            region=region,\n",
    "                            service_account_email = service_account_email\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_setup_wGCP(job_name):\n",
    "    GoogleCloudOptions.temp_location = temp_location\n",
    "    GoogleCloudOptions.staging_location = '{}/stage'.format(temp_location)\n",
    "    GoogleCloudOptions.project = project\n",
    "    GoogleCloudOptions.region = region\n",
    "    GoogleCloudOptions.job_name = job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_fetch(table_id):\n",
    "    SCHEMA = {}\n",
    "    table_sch = []\n",
    "    #in this try block it is attempting to get the table schema. its a side input\n",
    "    try:\n",
    "        SchemaJob = client.get_table('{}.{}.{}'.format(project,dataset,table_id))\n",
    "        for s in SchemaJob.schema:\n",
    "            new_dict = {}\n",
    "            new_dict['name'] = s.name\n",
    "            new_dict['type'] = s.field_type\n",
    "            new_dict['mode'] = s.mode\n",
    "            table_sch.append(new_dict)\n",
    "        SCHEMA['fields'] = table_sch\n",
    "        return SCHEMA\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline_csv(runner, beam_options,SCHEMA, job_name,filename,table_id):\n",
    "    table_sch = SCHEMA['fields']\n",
    "    p = beam.Pipeline(runner=runner,options=beam_options)              \n",
    "    rec, count  = (p | 'ReadData' >> beam.io.ReadFromText('{}/data.csv'.format(data_input), skip_header_lines =1)\n",
    "           | 'Split' >> beam.Map(lambda x: x.split(','))\n",
    "            | 'format to dict2' >> beam.ParDo(formating_csv(),table_sch, job_name).with_outputs(\"rec\", \"count\")\n",
    "                  )\n",
    "           \n",
    "    (count | 'Count total records' >> beam.combiners.Count.Globally()\n",
    "                     | 'Store count' >> beam.io.WriteToText('{}/count_temp.txt'.format(data_input), shard_name_template = \"\")\n",
    "            )\n",
    "    events  = (rec  | 'WriteToBigQuery2' >>  beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "               '{}:{}.{}'.format(project,dataset,table_id),\n",
    "               schema=SCHEMA,\n",
    "               write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                batch_size = 100000,\n",
    "               insert_retry_strategy=beam.io.gcp.bigquery_tools.RetryStrategy.RETRY_NEVER,\n",
    "               method='STREAMING_INSERTS')\n",
    "            )\n",
    "    (events[beam.io.gcp.bigquery.BigQueryWriteFn.FAILED_ROWS]\n",
    "             | \"remove tuple\" >> beam.ParDo(formating2(),table_id)\n",
    "            | \"Bad lines\" >> beam.io.WriteToText('{}/{}.txt'.format(bad_rows_input,filename), shard_name_template = \"\")\n",
    "    )\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline_avro(runner, beam_options,SCHEMA, job_name,filename,table_id):\n",
    "    table_sch = SCHEMA['fields']\n",
    "    p = beam.Pipeline(runner = runner,options=beam_options)              \n",
    "    rec, count  = (p | ReadFromAvro('{}/data.avro'.format(data_input))\n",
    "            | 'format to dict2' >> beam.ParDo(formating_avro(),table_sch, job_name).with_outputs(\"rec\", \"count\")\n",
    "              )\n",
    "    (count | 'Count total records' >> beam.combiners.Count.Globally()\n",
    "                     | 'Store count' >> beam.io.WriteToText('{}/count_temp.txt'.format(data_input), shard_name_template = \"\")\n",
    "            )\n",
    "    events= (rec| 'WriteToBigQuery2' >>  beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "               '{}:{}.{}'.format(project,dataset,table_id),\n",
    "               schema=SCHEMA,\n",
    "               write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                batch_size = 100000,\n",
    "               insert_retry_strategy=beam.io.gcp.bigquery_tools.RetryStrategy.RETRY_NEVER,\n",
    "               method='STREAMING_INSERTS')\n",
    "            )\n",
    "    (events[beam.io.gcp.bigquery.BigQueryWriteFn.FAILED_ROWS]\n",
    "             | \"remove tuple\" >> beam.ParDo(formating2(),table_id)\n",
    "            | \"Bad lines\" >> beam.io.WriteToText('{}/{}.txt'.format(bad_rows_input,filename), shard_name_template = \"\")\n",
    "    )\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline_json(runner, beam_options,SCHEMA, job_name,filename,table_id):\n",
    "    table_sch = SCHEMA['fields']\n",
    "    p = beam.Pipeline(runner = runner,options=beam_options)\n",
    "    rec, count  = (p | 'Read Data 1' >> beam.io.ReadFromText('{}/data.json'.format(data_input))\n",
    "             | 'Custom Parser 1' >> beam.ParDo(customJsonParse(),job_name).with_outputs(\"rec\", \"count\")\n",
    "              )\n",
    "    (count | 'Count total records' >> beam.combiners.Count.Globally()\n",
    "                     | 'Store count' >> beam.io.WriteToText('{}/count_temp.txt'.format(data_input), shard_name_template = \"\")\n",
    "            )\n",
    "    events = (rec| 'WriteToBigQuery2' >>  beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "               '{}:{}.{}'.format(project,dataset,table_id),\n",
    "                schema=SCHEMA,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                batch_size = 100000,\n",
    "               insert_retry_strategy=beam.io.gcp.bigquery_tools.RetryStrategy.RETRY_NEVER,\n",
    "               method='STREAMING_INSERTS')\n",
    "          )\n",
    "    (events[beam.io.gcp.bigquery.BigQueryWriteFn.FAILED_ROWS]\n",
    "             | \"remove tuple\" >> beam.ParDo(formating2(),table_id)\n",
    "            | \"Bad lines\" >> beam.io.WriteToText('{}/{}.txt'.format(bad_rows_input,filename), shard_name_template = \"\")\n",
    "    )\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline_parquet(runner,beam_options,SCHEMA, job_name,filename,table_id):\n",
    "    table_sch = SCHEMA['fields']\n",
    "    p = beam.Pipeline(runner = runner,options=beam_options)              \n",
    "    rec, count  = (p | 'ReadData' >> beam.io.ReadFromParquet('{}/data.parquet'.format(data_input))\n",
    "                | 'format to dict2' >> beam.ParDo(formating_parquet(),job_name).with_outputs(\"rec\", \"count\")\n",
    "              )\n",
    "    (count | 'Count total records' >> beam.combiners.Count.Globally()\n",
    "                     | 'Store count' >> beam.io.WriteToText('{}/count_temp.txt'.format(data_input), shard_name_template = \"\")\n",
    "            )\n",
    "    events = (rec| 'WriteToBigQuery2' >>  beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "               '{}:{}.{}'.format(project,dataset,table_id),\n",
    "               schema=SCHEMA,\n",
    "               write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                batch_size = 100000,\n",
    "               insert_retry_strategy=beam.io.gcp.bigquery_tools.RetryStrategy.RETRY_NEVER,\n",
    "               method='STREAMING_INSERTS')\n",
    "            )\n",
    "    (events[beam.io.gcp.bigquery.BigQueryWriteFn.FAILED_ROWS]\n",
    "             | \"remove tuple\" >> beam.ParDo(formating2(),table_id)\n",
    "            | \"Bad lines\" >> beam.io.WriteToText('{}/{}.txt'.format(bad_rows_input,filename), shard_name_template = \"\")\n",
    "    )\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(filename):\n",
    "    blob = bucket2.blob('{}.txt'.format(filename))\n",
    "    downloaded_blob = blob.download_as_string()\n",
    "    log_message = downloaded_blob.decode(\"utf-8\", \"ignore\")\n",
    "    log_message = log_message.replace(\"'\",'\"')\n",
    "    log_message = log_message.replace(\"None\", \"null\")\n",
    "    log_message = log_message.replace('b\"', \"b'\")\n",
    "    bad_list =log_message.split('\\n')\n",
    "    bad_json = []\n",
    "    bad_input_data = {}\n",
    "    for row in bad_list:\n",
    "        if row != '':\n",
    "            j = json.loads(row)\n",
    "            bad_json.append(j)\n",
    "    bad_input_data['elements'] = bad_json\n",
    "    return bad_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is needed to run the website html\n",
    "app = Flask(__name__)\n",
    "\n",
    "#secret key is to allow this python code to move variable from the frontend to the backend\n",
    "app.secret_key = \"35367565\"\n",
    "\n",
    "@app.route('/')\n",
    "def form():\n",
    "    return redirect(url_for('upload'))\n",
    "\n",
    "#This will render the frontend of the website to get the json file\n",
    "@app.route('/upload', methods=['GET','POST'])\n",
    "def upload():\n",
    "\n",
    "    #resets the message so that previous messages dont confuse the user\n",
    "    message = None\n",
    "    \n",
    "    #this will find all the tables being used in this project and display them as options to be used in bigquery\n",
    "    tables = ['Task3'] #change--------------------------\n",
    "    \n",
    "    #this creates a unique job name for dataflow\n",
    "    job_name = 'pythontobigquerry-{}'.format(uuid.uuid4())\n",
    "\n",
    "    beam_options = beam_setup(job_name)\n",
    "    #beam_options = beam_setup_wGCP(job_name)\n",
    "    runner = 'DataflowRunner'\n",
    "    #runner='DirectRunner'\n",
    "    \n",
    "    #this is needed to get the error logs from the bigquery in dataflow job\n",
    "    filter_str = (\n",
    "        f'resource.labels.job_name={job_name}'\n",
    "        f' resource.type=\"dataflow_step\"'\n",
    "        f' AND severity >= ERROR'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #This will only run if the user attempts to submit a file\n",
    "    if request.method == 'POST':\n",
    "\n",
    "        #this will only run if what the user submitted is a file\n",
    "        if 'file' in request.files:\n",
    "            #this gets the file data\n",
    "            file = request.files['file']\n",
    "            #this aquires the name of the file\n",
    "            filename = secure_filename(file.filename)\n",
    "            \n",
    "            #this will check it the file has data to be processed\n",
    "            if len(file.readlines()) == 0:\n",
    "                message = \"File has no data to process\"\n",
    "            else:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    #this will only run if the file is a csv\n",
    "                    if filename.endswith('.csv') or filename.endswith('.json') or filename.endswith('.avro') or filename.endswith('.parquet'):\n",
    "                        #these two lines are to upload the data fetched from the front-end to gcp cloud storage\n",
    "                        if filename.endswith('.csv'):\n",
    "                            blob = bucket.blob('data.csv')\n",
    "                            blob.upload_from_file(file, timeout=3600)\n",
    "                        elif filename.endswith('.avro'):\n",
    "                            blob = bucket.blob('data.avro')\n",
    "                            blob.upload_from_file(file, timeout=3600)\n",
    "                        elif filename.endswith('.json'):\n",
    "                            blob = bucket.blob('data.json')\n",
    "                            blob.upload_from_file(file, timeout=3600)\n",
    "                        elif filename.endswith('.parquet'):\n",
    "                            blob = bucket.blob('data.parquet')\n",
    "                            blob.upload_from_file(file, timeout=3600)\n",
    "                        \n",
    "                        #file.seek(0)\n",
    "                        #this gets the total number of records to be inserted to bigquery. its a side input\n",
    "                        #total_records = len(file.readlines())\n",
    "                        #total_records = 10\n",
    "                        \n",
    "                        #this gets the table wanting to be used from the front end\n",
    "                        table_id = request.form.getlist('checks')[0]\n",
    "                        SCHEMA = schema_fetch(table_id)\n",
    "                        \n",
    "                        #these two lines create a unique name for the files being saved to gcp cloud storage. files with the same name but in differnt buckets are tied together\n",
    "                        filename_helper = str(datetime.datetime.now())\n",
    "                        u_filename = \"LOG: \"+ filename_helper\n",
    "                        \n",
    "                        #in this try block, it will attempted to create the first pipeline that will read the input data and try to write it to bigquery. all failed rows with be stored into the gcp cloud storage\n",
    "                        try:\n",
    "                            start = time.time()\n",
    "                            if filename.endswith('.csv'):\n",
    "                                main_pipeline_csv(runner,beam_options,SCHEMA, job_name,u_filename,table_id)\n",
    "                            elif filename.endswith('.avro'):\n",
    "                                main_pipeline_avro(runner,beam_options,SCHEMA, job_name,u_filename,table_id)\n",
    "                            elif filename.endswith('.json'):\n",
    "                                main_pipeline_json(runner,beam_options,SCHEMA, job_name,u_filename,table_id)\n",
    "                            elif filename.endswith('.parquet'):\n",
    "                                main_pipeline_parquet(runner,beam_options,SCHEMA, job_name,u_filename,table_id)\n",
    "                                \n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up the data ingestion pipeline\"\n",
    "                            \n",
    "                        \n",
    "                        #this try block is creating a side input with all the bad rows attempted to be inserted into bigquery\n",
    "                        try:\n",
    "                            bad_input_data = get_errors(u_filename)\n",
    "                            errors = client.insert_rows_json('{}.{}.{}'.format(project,dataset,table_id), bad_input_data['elements'])\n",
    "                            error_message = ''\n",
    "                            with open('temp.txt', 'w') as m:\n",
    "                                for i in errors:\n",
    "                                    error_message =\"\"\"Error #{}, Message - {}, Location - {}, Inputted Data -  {}\\n\\n\"\"\".format(i['index'], i['errors'][0]['message'],i['errors'][0]['location'],bad_input_data['elements'][i['index']])\n",
    "                                    m.write(error_message)\n",
    "                            blob = bucket3.blob('{}.txt'.format(u_filename))\n",
    "                            blob.upload_from_filename(local_file_loc, timeout=3600)\n",
    "                            os.remove(\"temp.txt\")\n",
    "                            message = \"Data uploaded to the Bigquery\"\n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up messaging\"\n",
    "                        \n",
    "                        \n",
    "                        blob = bucket.blob('count_temp.txt')\n",
    "                        total_records = blob.download_as_string()\n",
    "                        total_records = total_records.decode(\"utf-8\", \"ignore\")\n",
    "                        end = time.time()\n",
    "                        Total_time = end - start\n",
    "                        try:\n",
    "                            # Good elements - read number of elements in BigQuery table\n",
    "                            query = \"\"\"SELECT count(*) FROM `{}.{}.{}` WHERE job_id = '{}'\"\"\".format(str(project),str(dataset),str(table_id),job_name)\n",
    "                            results =  client.query(query)\n",
    "                        except Exception as e:\n",
    "                                message = \"Error getting the query\"\n",
    "                                print(\"ERROR: could not query\")\n",
    "                        try:\n",
    "                            num_good = \"\"\n",
    "                            for row in results:\n",
    "                                num_good = row[\"f0_\"]\n",
    "                            #Write to GCP bucket\n",
    "                            msg = '''{}, {} seconds, {}, {}, {}\\n'''.format(str(job_name),str(Total_time), str(total_records.strip()), str(num_good), str(len(bad_input_data['elements'])))\n",
    "                            if os.path.exists('metrics.csv'):\n",
    "                                with open('metrics.csv', 'a') as f:\n",
    "                                    f.write(msg)\n",
    "                            else:\n",
    "                                with open('metrics.csv', 'w') as f:\n",
    "                                    f.write('Project Id, Execution Time, Total Rows, Total good Rows, Total Bad Rows\\n')\n",
    "                                    f.write(msg)\n",
    "                        except Exception as e:\n",
    "                                message = \"Error writting results\"\n",
    "                                print(e)\n",
    "                                \n",
    "                    #If the file is not a json or the csv this will run\n",
    "                    else:\n",
    "                        message = \"File type is not excepted\"\n",
    "                    #endif\n",
    "                except Exception as error:\n",
    "                    print('This was the error: ', error)\n",
    "                    message = \"There was an error in creating the request\"\n",
    "            #endif\n",
    "        #This will run if the submition is not a file type\n",
    "        elif 'file' not in request.files:\n",
    "            message = \"There was no file to upload\"\n",
    "        #endif\n",
    "    #endif\n",
    "\n",
    "    #this will render the template on the website\n",
    "    return render_template(\"front.html\", message = message, tables = tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Apr/2022 15:41:27] \"\u001b[32mGET / HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [27/Apr/2022 15:41:27] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Apr/2022 15:41:27] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad\\anaconda3\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery.py:2138: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  is_streaming_pipeline = p.options.view_as(StandardOptions).streaming\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-57b9e56e-3ee4-4627-af6a-5fe07943e848.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "ERROR:apache_beam.io.gcp.bigquery:There were errors inserting to BigQuery. Will not retry. Errors were [{'index': 3, 'errors': [{'reason': 'invalid', 'location': 'num', 'debugInfo': '', 'message': 'NUMERIC(10) has precision 10 and scale 0 but got a value that is not in range of [-9999999999, 9999999999]'}]}, {'index': 4, 'errors': [{'reason': 'invalid', 'location': 'limit', 'debugInfo': '', 'message': 'Field limit: STRING(5) has maximum length 5 but got a value with length 12'}]}, {'index': 5, 'errors': [{'reason': 'invalid', 'location': 'num', 'debugInfo': '', 'message': 'Invalid NUMERIC value: 1e+41'}]}, {'index': 6, 'errors': [{'reason': 'invalid', 'location': 'bytes', 'debugInfo': '', 'message': 'Failed to decode bytes input. Byte fields must be base64 encoded, value: @@@@#####)(*&%$.'}]}, {'index': 7, 'errors': [{'reason': 'invalid', 'location': 'time', 'debugInfo': '', 'message': \"Could not parse 'nowtime' as a timestamp. Required format is YYYY-MM-DD HH:MM[:SS[.SSSSSS]]\"}]}, {'index': 8, 'errors': [{'reason': 'invalid', 'location': 'geo', 'debugInfo': '', 'message': \"Cannot convert value here to GEOGRAPHY: Unexpected 'here' at position 0.\"}]}, {'index': 9, 'errors': [{'reason': 'invalid', 'location': 'new', 'debugInfo': '', 'message': 'Cannot convert string value to boolean: old'}]}, {'index': 11, 'errors': [{'reason': 'invalid', 'location': 'salary_in_k', 'debugInfo': '', 'message': 'Field value of salary_in_k cannot be empty.'}]}]\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "127.0.0.1 - - [27/Apr/2022 15:41:42] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Apr/2022 15:41:42] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
