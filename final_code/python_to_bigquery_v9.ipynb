{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project: Import csv to bigquery table\n",
    "#StartDate: 4/06/2022\n",
    "#EndDate: \n",
    "#Developer: Bradley, Hongquy, Khoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,render_template,request,redirect,url_for\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "import os, re, datetime, uuid, time, json\n",
    "import google.cloud.logging\n",
    "import apache_beam as beam\n",
    "from io import StringIO\n",
    "import argparse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from werkzeug.utils import secure_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to allow a authenticated user on the project to work on bigquerry\n",
    "Key_path = r\"C:\\\\Users\\Brad\\Documents\\model-craft-342921-ea36cdb339e7.json\"\n",
    "\n",
    "#this is needed to request a job\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = Key_path\n",
    "credentials = service_account.Credentials.from_service_account_file(Key_path,scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "client=bigquery.Client(credentials=credentials,project=credentials.project_id)\n",
    "\n",
    "#this is needed to allow the request to access gcp cloud storage\n",
    "storage_client=storage.Client(credentials=credentials,project=credentials.project_id)\n",
    "\n",
    "#this is where a temporary file is stored to be accessed by the cloud\n",
    "bucket = storage_client.get_bucket(\"data_intake4062022\")\n",
    "\n",
    "#this is where the column location and message from bigquery is stored to be accessed later\n",
    "bucket1 = storage_client.get_bucket(\"error_messages4142022\")\n",
    "\n",
    "#this is where the rows attempted to be inserted into bigquery is stored to be accessed later\n",
    "bucket2 = storage_client.get_bucket(\"practice_error_logs\")\n",
    "\n",
    "bucket3 = storage_client.get_bucket(\"job_details_4192022\")\n",
    "\n",
    "#this is needed to access logs from the bigquery job\n",
    "client1 = google.cloud.logging.Client()\n",
    "logger = client1.logger(name=\"dataflow.googleapis.com%2Fworker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a function created to format the input data into a dictionary so the data can be easily inserted into bigquery\n",
    "class formating(beam.DoFn):\n",
    "    #table_sch is a side input that was found for the table we are trying to insert the data into\n",
    "    def process(self, element, table_sch, job_name):\n",
    "        import apache_beam as beam\n",
    "        record = {}\n",
    "        record['job_id'] = job_name\n",
    "        for x in range(0, len(element)):\n",
    "            name = table_sch[x]['name']\n",
    "            record[name] = element[x]\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is for the second pipeline and will merge the message from bigquery with its respective input data\n",
    "class process_message(beam.DoFn):\n",
    "    #bad_input_data is a side input that contains all the bad inserts from bigquery\n",
    "    #Total is a side input that will be used for numbering the error message\n",
    "    def process(self, element, bad_input_data, count):\n",
    "        import apache_beam as beam\n",
    "        from io import StringIO\n",
    "        import json, re\n",
    "        #these few steps are required to format the data\n",
    "        bad_message = element.replace(\"'\",'\"')\n",
    "        bad_message = json.loads(bad_message)\n",
    "        #the last steps take an element from the pipeline, which is the message and column location, and match it with the side input\n",
    "        for bad_message_row in bad_message['elements']:\n",
    "            for input_data in bad_input_data['elements']:\n",
    "                message = ''\n",
    "                if input_data[bad_message_row['location']] == bad_message_row['message'].split(': ')[1]:\n",
    "                    count = count+1\n",
    "                    #this is the final message that will be seen by the user\n",
    "                    message = 'Error #{} - Message: {} in column {} - Input: {}\\n'.format(count, bad_message_row['message'],bad_message_row['location'],input_data)\n",
    "                    #this will remove the used inputed row data so it wont be used more than once\n",
    "                    bad_input_data['elements'].remove(input_data)\n",
    "                    yield message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = 'LOG: 2022-04-19 17:42:59.419482.txt'\n",
    "job_name = 'pythontobigquerry-{}'.format(uuid.uuid4())\n",
    "try:\n",
    "    blob = bucket2.blob(filename)\n",
    "    downloaded_blob = blob.download_as_string()\n",
    "    log_message = downloaded_blob.decode(\"utf-8\", \"ignore\")\n",
    "    log_message =log_message.split('\\n')\n",
    "    bad_row = []\n",
    "    bad_json = []\n",
    "    bad_input_data = {}\n",
    "    for element in log_message:\n",
    "        element = element.replace(\"'model-craft-342921:testing.Task3', \",'')\n",
    "        element = element.replace('(','')\n",
    "        element = element.replace(')','')\n",
    "        element = element.replace(\"'\",'\"')\n",
    "        bad_row.append(element)\n",
    "    for index in range(0,len(bad_row)-1):\n",
    "        bad_json.append(json.loads(bad_row[index]))\n",
    "    bad_input_data['elements'] = bad_json\n",
    "\n",
    "except Exception as error:\n",
    "    print('This was the error: ', error)\n",
    "count = 0\n",
    "beam_options = PipelineOptions(\n",
    "                            runner = 'DataflowRunner',\n",
    "                            #runner='DirectRunner',\n",
    "                            project = 'model-craft-342921',\n",
    "                            job_name = '{}'.format(job_name),\n",
    "                            temp_location = 'gs://data_intake4062022/temp1',\n",
    "                            region='us-east1',\n",
    "                            service_account_email = 'practice-py@model-craft-342921.iam.gserviceaccount.com'\n",
    "                        )\n",
    "try:\n",
    "    p1 = beam.Pipeline(options=beam_options)\n",
    "    events  = (p1 | 'ReadData' >> beam.io.ReadFromText('gs://error_messages4142022/{}'.format(filename))\n",
    "                  | 'Process Messaging' >> beam.ParDo(process_message(),bad_input_data,count)\n",
    "                    | 'Store Final Message' >> beam.io.WriteToText('gs://complete_message_4182022/{}'.format(filename), shard_name_template = \"\")\n",
    "              )\n",
    "    result = p1.run()\n",
    "    result.wait_until_finish()\n",
    "\n",
    "except Exception as error:\n",
    "    print('This was the error: ', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is needed to run the website html\n",
    "app = Flask(__name__)\n",
    "\n",
    "#secret key is to allow this python code to move variable from the frontend to the backend\n",
    "app.secret_key = \"35367565\"\n",
    "\n",
    "@app.route('/')\n",
    "def form():\n",
    "    return redirect(url_for('upload'))\n",
    "\n",
    "#This will render the frontend of the website to get the json file\n",
    "@app.route('/upload', methods=['GET','POST'])\n",
    "def upload():\n",
    "\n",
    "    #resets the message so that previous messages dont confuse the user\n",
    "    message = None\n",
    "    \n",
    "    #this will find all the tables being used in this project and display them as options to be used in bigquery\n",
    "    tables = ['Task3'] #change--------------------------\n",
    "    \n",
    "    #this creates a unique job name for dataflow\n",
    "    job_name = 'pythontobigquerry-{}'.format(uuid.uuid4())\n",
    "\n",
    "    beam_options = PipelineOptions(\n",
    "                                runner = 'DataflowRunner',\n",
    "                                #runner='DirectRunner',\n",
    "                                project = 'model-craft-342921',\n",
    "                                job_name = '{}'.format(job_name),\n",
    "                                temp_location = 'gs://data_intake4062022/temp1',\n",
    "                                region='us-east1',\n",
    "                                service_account_email = 'practice-py@model-craft-342921.iam.gserviceaccount.com'\n",
    "                            )\n",
    "    \n",
    "    #this is needed to get the error logs from the bigquery in dataflow job\n",
    "    filter_str = (\n",
    "        f'resource.labels.job_name={job_name}'\n",
    "        f' resource.type=\"dataflow_step\"'\n",
    "        f' AND severity >= ERROR'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #This will only run if the user attempts to submit a file\n",
    "    if request.method == 'POST':\n",
    "\n",
    "        #this will only run if what the user submitted is a file\n",
    "        if 'file' in request.files:\n",
    "            #this gets the file data\n",
    "            file = request.files['file']\n",
    "            #this aquires the name of the file\n",
    "            filename = secure_filename(file.filename)\n",
    "            \n",
    "            #this will check it the file has data to be processed\n",
    "            if len(file.readlines()) == 0:\n",
    "                message = \"File has no data to process\"\n",
    "            else:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    #this will only run if the file is a csv\n",
    "                    if filename.endswith('.csv'):\n",
    "                        \n",
    "                        #these two lines are to upload the data fetched from the front-end to gcp cloud storage\n",
    "                        blob = bucket.blob('data.csv')\n",
    "                        blob.upload_from_file(file, timeout=3600)\n",
    "                        \n",
    "                        \n",
    "                        file.seek(0)\n",
    "                        #this gets the total number of records to be inserted to bigquery. its a side input\n",
    "                        total_records = len(file.readlines())\n",
    "                        \n",
    "                        #this gets the table wanting to be used from the front end\n",
    "                        table_id = request.form.getlist('checks')[0]\n",
    "                        SCHEMA = {}\n",
    "                        table_sch = []\n",
    "                        #in this try block it is attempting to get the table schema. its a side input\n",
    "                        try:\n",
    "                            SchemaJob = client.get_table('model-craft-342921.testing.{}'.format(table_id))\n",
    "                            for s in SchemaJob.schema:\n",
    "                                new_dict = {}\n",
    "                                new_dict['name'] = s.name\n",
    "                                new_dict['type'] = s.field_type\n",
    "                                new_dict['mode'] = s.mode\n",
    "                                table_sch.append(new_dict)\n",
    "                            SCHEMA['fields'] = table_sch\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                        #these two lines create a unique name for the files being saved to gcp cloud storage. files with the same name but in differnt buckets are tied together\n",
    "                        filename_helper = str(datetime.datetime.now())\n",
    "                        filename = \"LOG: \"+ filename_helper + \".txt\"\n",
    "                        \n",
    "                        #in this try block, it will attempted to create the first pipeline that will read the input data and try to write it to bigquery. all failed rows with be stored into the gcp cloud storage\n",
    "                        try:\n",
    "                            start = time.time()\n",
    "                            p = beam.Pipeline(options=beam_options)\n",
    "                            events  = (p | 'ReadData' >> beam.io.ReadFromText('gs://data_intake4062022/data.csv', skip_header_lines =1)\n",
    "                                   | 'Split' >> beam.Map(lambda x: x.split(','))\n",
    "                                    | 'format to dict2' >> beam.ParDo(formating(),table_sch, job_name) \n",
    "                                   | 'WriteToBigQuery2' >>  beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "                                       'model-craft-342921:testing.{}'.format(table_id),\n",
    "                                       schema=SCHEMA,\n",
    "                                       write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                                       insert_retry_strategy=beam.io.gcp.bigquery_tools.RetryStrategy.RETRY_NEVER,\n",
    "                                       method='STREAMING_INSERTS')\n",
    "                                    )\n",
    "                            (events[beam.io.gcp.bigquery.BigQueryWriteFn.FAILED_ROWS]\n",
    "                                    | \"Bad lines\" >> beam.io.WriteToText('gs://practice_error_logs/{}'.format(filename), shard_name_template = \"\")\n",
    "                            )\n",
    "                            \n",
    "                            result = p.run()\n",
    "                            result.wait_until_finish()\n",
    "                            \n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up the data ingestion pipeline\"\n",
    "                        \n",
    "                        #this try block will attempted to fetch the logs needed to complete the erroring messaging system for bad row inserts\n",
    "                        try:\n",
    "                            log = {}\n",
    "                            log_builder = {}\n",
    "                            log_array = []\n",
    "                            for entry in logger.list_entries(filter_=filter_str):  # API call(s)\n",
    "                                #these few lines will extract the needed data and get rid of everything that isnt column location and bigquery response message\n",
    "                                message = entry.to_api_repr()['jsonPayload']['message']\n",
    "                                list_extract = message.replace(\"There were errors inserting to BigQuery. Will not retry. Errors were \",\"\")\n",
    "                                list_extract = list_extract.replace(\"'\",'\"')\n",
    "                                res = json.loads(list_extract)\n",
    "                                #these few lines will save the needed data into a dictionary\n",
    "                                for i in range(0,len(res)):\n",
    "                                    log_builder = {}\n",
    "                                    log_builder['location'] = res[i]['errors'][0]['location']\n",
    "                                    log_builder['message'] = res[i]['errors'][0]['message']\n",
    "                                    log_array.append(log_builder)\n",
    "                            log['elements'] = log_array\n",
    "                            with open('temp.txt', 'a') as outfile:\n",
    "                                outfile.write(json.dumps(log)+'\\n')\n",
    "                            #these few lines are uploading the temp file to gcp cloud storage\n",
    "                            blob = bucket1.blob(filename)\n",
    "                            blob.upload_from_filename(r\"C:\\\\Users\\Brad\\temp.txt\", timeout=3600)\n",
    "                            total_bad_records = len(log_array)\n",
    "                            os.remove(\"temp.txt\")\n",
    "                            message = \"Data uploaded to the Bigquery\"\n",
    "                            \n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error getting logs\"\n",
    "                        #this try block is creating a side input with all the bad rows attempted to be inserted into bigquery\n",
    "                        try:\n",
    "                            blob = bucket2.blob(filename)\n",
    "                            downloaded_blob = blob.download_as_string()\n",
    "                            log_message = downloaded_blob.decode(\"utf-8\", \"ignore\")\n",
    "                            log_message =log_message.split('\\n')\n",
    "                            bad_row = []\n",
    "                            bad_json = []\n",
    "                            bad_input_data = {}\n",
    "                            for element in log_message:\n",
    "                                element = element.replace(\"'model-craft-342921:testing.{}', \".format(table_id),'')\n",
    "                                element = element.replace('(','')\n",
    "                                element = element.replace(')','')\n",
    "                                element = element.replace(\"'\",'\"')\n",
    "                                bad_row.append(element)\n",
    "                            for index in range(0,len(bad_row)-1):\n",
    "                                bad_json.append(json.loads(bad_row[index]))\n",
    "                            bad_input_data['elements'] = bad_json\n",
    "\n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up messaging\"\n",
    "                            \n",
    "                        \n",
    "                        #in this try block, this is the second pipeline that generates the final message that will be read by the user\n",
    "                        try:\n",
    "                            count =0\n",
    "                            p1 = beam.Pipeline(options=beam_options)\n",
    "                            events  = (p1 | 'ReadData' >> beam.io.ReadFromText('gs://error_messages4142022/{}'.format(filename))\n",
    "                                           | 'Process Messaging' >> beam.ParDo(process_message(),bad_input_data,count)\n",
    "                                            | 'Store Final Message' >> beam.io.WriteToText('gs://complete_message_4182022/{}'.format(filename), shard_name_template = \"\")\n",
    "                                      )\n",
    "                            result = p1.run()\n",
    "                            result.wait_until_finish()\n",
    "                        \n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up the error formating pipeline\"\n",
    "                        \n",
    "                        end = time.time()\n",
    "                        Total_time = end - start\n",
    "                        \n",
    "                        # Good elements - read number of elements in BigQuery table\n",
    "                        query = \"\"\"SELECT count(*) FROM `model-craft-342921.testing.{}` WHERE job_id = '{}'\"\"\".format(str(table_id),job_name)\n",
    "                        try:\n",
    "                            results =  client.query(query)\n",
    "                        except Exception as e:\n",
    "                                print(\"ERROR: could not query\")\n",
    "                        num_good = \"\"\n",
    "                        for row in results:\n",
    "                            num_good = row[\"f0_\"]\n",
    "                        #Write to GCP bucket\n",
    "                        msg = '''Time elasped: {} seconds, Number of Rows: {}, Number of Valid Rows: {}, Number of Invalid Rows: {}\\n'''.format(str(Total_time), str(total_records-1), str(num_good), str(total_bad_records))\n",
    "\n",
    "                        blob = bucket3.blob(\"metrics\")\n",
    "                        blob.upload_from_string(msg, timeout=3600)\n",
    "                    #If the file is not a json or the csv this will run\n",
    "                    else:\n",
    "                        message = \"File type is not excepted\"\n",
    "                    #endif\n",
    "                except Exception as error:\n",
    "                    print('This was the error: ', error)\n",
    "                    message = \"There was an error in creating the request\"\n",
    "            #endif\n",
    "        #This will run if the submition is not a file type\n",
    "        elif 'file' not in request.files:\n",
    "            message = \"There was no file to upload\"\n",
    "        #endif\n",
    "    #endif\n",
    "\n",
    "    #this will render the template on the website\n",
    "    return render_template(\"front.html\", message = message, tables = tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [20/Apr/2022 15:34:44] \"\u001b[32mGET / HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [20/Apr/2022 15:34:44] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [20/Apr/2022 15:34:44] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad\\anaconda3\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery.py:2138: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  is_streaming_pipeline = p.options.view_as(StandardOptions).streaming\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-3f3b04a2-c791-4ddd-b674-b3748e587139.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-3f3b04a2-c791-4ddd-b674-b3748e587139.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-3f3b04a2-c791-4ddd-b674-b3748e587139.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-3f3b04a2-c791-4ddd-b674-b3748e587139.json']\n",
      "127.0.0.1 - - [20/Apr/2022 16:24:00] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [20/Apr/2022 16:24:00] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
