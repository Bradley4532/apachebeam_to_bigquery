{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project: Import csv to bigquery table\n",
    "#StartDate: 4/06/2022\n",
    "#EndDate: \n",
    "#Developer: Bradley, Hongquy, Khoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,render_template,request,redirect,url_for\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "import os, re, datetime, uuid, time, json\n",
    "import google.cloud.logging\n",
    "import apache_beam as beam\n",
    "from io import StringIO\n",
    "import argparse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from werkzeug.utils import secure_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'model-craft-342921'\n",
    "temp_location = 'gs://data_intake4062022/temp1'\n",
    "region='us-east1'\n",
    "service_account_email = 'practice-py@model-craft-342921.iam.gserviceaccount.com'\n",
    "dataset = 'testing'\n",
    "data_input = 'gs://data_intake4062022/data.csv'\n",
    "message_ouput = 'gs://complete_message_4182022'\n",
    "bad_rows_input = 'gs://practice_error_logs'\n",
    "local_file_loc = r\"C:\\\\Users\\Brad\\temp.txt\"\n",
    "error_message = 'gs://error_messages4142022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to allow a authenticated user on the project to work on bigquerry\n",
    "Key_path = r\"C:\\\\Users\\Brad\\Documents\\model-craft-342921-ea36cdb339e7.json\"\n",
    "\n",
    "#this is needed to request a job\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = Key_path\n",
    "credentials = service_account.Credentials.from_service_account_file(Key_path,scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "client=bigquery.Client(credentials=credentials,project=credentials.project_id)\n",
    "\n",
    "#this is needed to allow the request to access gcp cloud storage\n",
    "storage_client=storage.Client(credentials=credentials,project=credentials.project_id)\n",
    "\n",
    "#this is where a temporary file is stored to be accessed by the cloud\n",
    "bucket = storage_client.get_bucket(\"data_intake4062022\")\n",
    "\n",
    "#this is where the column location and message from bigquery is stored to be accessed later\n",
    "bucket1 = storage_client.get_bucket(\"error_messages4142022\")\n",
    "\n",
    "#this is where the rows attempted to be inserted into bigquery is stored to be accessed later\n",
    "bucket2 = storage_client.get_bucket(\"practice_error_logs\")\n",
    "\n",
    "bucket3 = storage_client.get_bucket(\"complete_message_4182022\")\n",
    "\n",
    "#this is needed to access logs from the bigquery job\n",
    "client1 = google.cloud.logging.Client()\n",
    "logger = client1.logger(name=\"dataflow.googleapis.com%2Fworker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a function created to format the input data into a dictionary so the data can be easily inserted into bigquery\n",
    "class formating(beam.DoFn):\n",
    "    #table_sch is a side input that was found for the table we are trying to insert the data into\n",
    "    def process(self, element, table_sch, job_name):\n",
    "        import apache_beam as beam\n",
    "        record = {}\n",
    "        record['job_id'] = job_name\n",
    "        for x in range(0, len(element)):\n",
    "            if x < len(table_sch)-1:\n",
    "                name = table_sch[x]['name']\n",
    "                if element[x] == '':\n",
    "                    record[name] = None\n",
    "                else:\n",
    "                    record[name] = element[x]\n",
    "            else:\n",
    "                record['unknown{}'.format(x)] = element[x]\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class formating2(beam.DoFn):\n",
    "    def process(self, element, table_id):\n",
    "        import apache_beam as beam\n",
    "        element = element[1]\n",
    "        element.pop('job_id', None)\n",
    "        yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_fetch(table_id):\n",
    "    SCHEMA = {}\n",
    "    table_sch = []\n",
    "    #in this try block it is attempting to get the table schema. its a side input\n",
    "    try:\n",
    "        SchemaJob = client.get_table('{}.{}.{}'.format(project,dataset,table_id))\n",
    "        for s in SchemaJob.schema:\n",
    "            new_dict = {}\n",
    "            new_dict['name'] = s.name\n",
    "            new_dict['type'] = s.field_type\n",
    "            new_dict['mode'] = s.mode\n",
    "            table_sch.append(new_dict)\n",
    "        SCHEMA['fields'] = table_sch\n",
    "        return SCHEMA\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(beam_options,SCHEMA, job_name,filename,table_id):\n",
    "    table_sch = SCHEMA['fields']\n",
    "    p = beam.Pipeline(options=beam_options)              \n",
    "    events  = (p | 'ReadData' >> beam.io.ReadFromText(data_input, skip_header_lines =1)\n",
    "           | 'Split' >> beam.Map(lambda x: x.split(','))\n",
    "            | 'format to dict2' >> beam.ParDo(formating(),table_sch, job_name)\n",
    "            | 'WriteToBigQuery2' >>  beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "               '{}:{}.{}'.format(project,dataset,table_id),\n",
    "               schema=SCHEMA,\n",
    "               write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                batch_size = 100000,\n",
    "               insert_retry_strategy=beam.io.gcp.bigquery_tools.RetryStrategy.RETRY_NEVER,\n",
    "               method='STREAMING_INSERTS')\n",
    "            )\n",
    "    (events[beam.io.gcp.bigquery.BigQueryWriteFn.FAILED_ROWS]\n",
    "             | \"remove tuple\" >> beam.ParDo(formating2(),table_id)\n",
    "            | \"Bad lines\" >> beam.io.WriteToText('{}/{}.txt'.format(bad_rows_input,filename), append_trailing_newlines=True, shard_name_template = \"\")\n",
    "    )\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(filename):\n",
    "    blob = bucket2.blob('{}.txt'.format(filename))\n",
    "    downloaded_blob = blob.download_as_string()\n",
    "    log_message = downloaded_blob.decode(\"utf-8\", \"ignore\")\n",
    "    log_message = log_message.replace(\"'\",'\"')\n",
    "    log_message = log_message.replace(\"None\", \"null\")\n",
    "    bad_list =log_message.split('\\n')\n",
    "    bad_json = []\n",
    "    bad_input_data = {}\n",
    "    for row in bad_list:\n",
    "        if row != '':\n",
    "            j = json.loads(row)\n",
    "            bad_json.append(j)\n",
    "    bad_input_data['elements'] = bad_json\n",
    "    return bad_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is needed to run the website html\n",
    "app = Flask(__name__)\n",
    "\n",
    "#secret key is to allow this python code to move variable from the frontend to the backend\n",
    "app.secret_key = \"35367565\"\n",
    "\n",
    "@app.route('/')\n",
    "def form():\n",
    "    return redirect(url_for('upload'))\n",
    "\n",
    "#This will render the frontend of the website to get the json file\n",
    "@app.route('/upload', methods=['GET','POST'])\n",
    "def upload():\n",
    "\n",
    "    #resets the message so that previous messages dont confuse the user\n",
    "    message = None\n",
    "    \n",
    "    #this will find all the tables being used in this project and display them as options to be used in bigquery\n",
    "    tables = ['Task3'] #change--------------------------\n",
    "    \n",
    "    #this creates a unique job name for dataflow\n",
    "    job_name = 'pythontobigquerry-{}'.format(uuid.uuid4())\n",
    "\n",
    "    beam_options = PipelineOptions(\n",
    "                                runner = 'DataflowRunner',\n",
    "                                #runner='DirectRunner',\n",
    "                                project = project,\n",
    "                                job_name = '{}'.format(job_name),\n",
    "                                temp_location = temp_location,\n",
    "                                region=region,\n",
    "                                service_account_email = service_account_email\n",
    "                            )\n",
    "    \n",
    "    #this is needed to get the error logs from the bigquery in dataflow job\n",
    "    filter_str = (\n",
    "        f'resource.labels.job_name={job_name}'\n",
    "        f' resource.type=\"dataflow_step\"'\n",
    "        f' AND severity >= ERROR'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #This will only run if the user attempts to submit a file\n",
    "    if request.method == 'POST':\n",
    "\n",
    "        #this will only run if what the user submitted is a file\n",
    "        if 'file' in request.files:\n",
    "            #this gets the file data\n",
    "            file = request.files['file']\n",
    "            #this aquires the name of the file\n",
    "            filename = secure_filename(file.filename)\n",
    "            \n",
    "            #this will check it the file has data to be processed\n",
    "            if len(file.readlines()) == 0:\n",
    "                message = \"File has no data to process\"\n",
    "            else:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    #this will only run if the file is a csv\n",
    "                    if filename.endswith('.csv'):\n",
    "                        \n",
    "                        #these two lines are to upload the data fetched from the front-end to gcp cloud storage\n",
    "                        blob = bucket.blob('data.csv')\n",
    "                        blob.upload_from_file(file, timeout=3600)\n",
    "                        \n",
    "                        file.seek(0)\n",
    "                        #this gets the total number of records to be inserted to bigquery. its a side input\n",
    "                        total_records = len(file.readlines())\n",
    "                        \n",
    "                        #this gets the table wanting to be used from the front end\n",
    "                        table_id = request.form.getlist('checks')[0]\n",
    "                        SCHEMA = schema_fetch(table_id)\n",
    "                        \n",
    "                        #these two lines create a unique name for the files being saved to gcp cloud storage. files with the same name but in differnt buckets are tied together\n",
    "                        filename_helper = str(datetime.datetime.now())\n",
    "                        filename = \"LOG: \"+ filename_helper\n",
    "                        \n",
    "                        #in this try block, it will attempted to create the first pipeline that will read the input data and try to write it to bigquery. all failed rows with be stored into the gcp cloud storage\n",
    "                        try:\n",
    "                            start = time.time()\n",
    "                            main_pipeline(beam_options,SCHEMA, job_name,filename,table_id)\n",
    "\n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up the data ingestion pipeline\"\n",
    "                            \n",
    "                        \n",
    "                        #this try block is creating a side input with all the bad rows attempted to be inserted into bigquery\n",
    "                        try:\n",
    "                            bad_input_data = get_errors(filename)\n",
    "                            errors = client.insert_rows_json('{}.{}.{}'.format(project,dataset,table_id), bad_input_data['elements'])\n",
    "                            error_message = ''\n",
    "                            with open('temp.txt', 'w') as m:\n",
    "                                for i in errors:\n",
    "                                    error_message =\"\"\"Error #{}, Message - {}, Location - {}, Inputted Data -  {}\\n\\n\"\"\".format(i['index'], i['errors'][0]['message'],i['errors'][0]['location'],bad_input_data['elements'][i['index']])\n",
    "                                    m.write(error_message)\n",
    "                            blob = bucket3.blob('{}.txt'.format(filename))\n",
    "                            blob.upload_from_filename(local_file_loc, timeout=3600)\n",
    "                            os.remove(\"temp.txt\")\n",
    "                            message = \"Data uploaded to the Bigquery\"\n",
    "                        except Exception as error:\n",
    "                            print('This was the error: ', error)\n",
    "                            message = \"Error setting up messaging\"\n",
    "\n",
    "                        \n",
    "                        end = time.time()\n",
    "                        Total_time = end - start\n",
    "                        \n",
    "                        # Good elements - read number of elements in BigQuery table\n",
    "                        query = \"\"\"SELECT count(*) FROM `{}.{}.{}` WHERE job_id = '{}'\"\"\".format(str(project),str(dataset),str(table_id),job_name)\n",
    "                        try:\n",
    "                            results =  client.query(query)\n",
    "                        except Exception as e:\n",
    "                                print(\"ERROR: could not query\")\n",
    "                        num_good = \"\"\n",
    "                        for row in results:\n",
    "                            num_good = row[\"f0_\"]\n",
    "                        #Write to GCP bucket\n",
    "                        msg = '''{}, {} seconds, {}, {}, {}\\n'''.format(str(job_name),str(Total_time), str(total_records-1), str(num_good), str(len(bad_input_data['elements'])))\n",
    "                        if os.path.exists('metrics.csv'):\n",
    "                            with open('metrics.csv', 'a') as f:\n",
    "                                f.write(msg)\n",
    "                        else:\n",
    "                            with open('metrics.csv', 'w') as f:\n",
    "                                f.write('Project Id, Execution Time, Total Rows, Total good Rows, Total Bad Rows\\n')\n",
    "                                f.write(msg)\n",
    "                    \n",
    "                    #If the file is not a json or the csv this will run\n",
    "                    else:\n",
    "                        message = \"File type is not excepted\"\n",
    "                    #endif\n",
    "                except Exception as error:\n",
    "                    print('This was the error: ', error)\n",
    "                    message = \"There was an error in creating the request\"\n",
    "            #endif\n",
    "        #This will run if the submition is not a file type\n",
    "        elif 'file' not in request.files:\n",
    "            message = \"There was no file to upload\"\n",
    "        #endif\n",
    "    #endif\n",
    "\n",
    "    #this will render the template on the website\n",
    "    return render_template(\"front.html\", message = message, tables = tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [25/Apr/2022 14:47:54] \"\u001b[32mGET / HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [25/Apr/2022 14:47:54] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Apr/2022 14:47:54] \"\u001b[37mGET /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad\\anaconda3\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery.py:2138: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  is_streaming_pipeline = p.options.view_as(StandardOptions).streaming\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-0e59b36c-a0d8-4a33-b1fd-979cb9fe9adc.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\Brad\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-0e59b36c-a0d8-4a33-b1fd-979cb9fe9adc.json']\n",
      "127.0.0.1 - - [25/Apr/2022 14:53:36] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Apr/2022 14:53:36] \"\u001b[37mPOST /upload HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
